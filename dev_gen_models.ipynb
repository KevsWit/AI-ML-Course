{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9813b2df",
        "outputId": "06ded7d3-2fb2-4865-9f6c-6dce2b7778ea"
      },
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the JSONL file into a pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_json('quotes.jsonl', lines=True)\n",
        "\n",
        "    # Convert the pandas DataFrame to a datasets Dataset\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    # Now you can access the data like before\n",
        "    print(dataset['train'][0])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: quotes.jsonl not found. Please make sure the file is uploaded to your Colab environment.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: \"Column train not in the dataset. Current columns in the dataset: ['quote', 'author', 'tags']\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ff4870c",
        "outputId": "b469fb6e-1652-4aea-f782-737b34965cc8"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "# Extract the quotes from the dataset and combine them into a single string\n",
        "data = \" \".join(dataset['quote'])\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "sequences = tokenizer.texts_to_sequences([data])\n",
        "\n",
        "# Pad sequences\n",
        "# Find the maximum sequence length among all sequences\n",
        "max_sequence_len = max([len(x) for x in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "print(\"Original data snippet:\", data[:500] + \"...\")\n",
        "print(\"Number of sequences:\", len(sequences))\n",
        "print(\"Max sequence length:\", max_sequence_len)\n",
        "print(\"Shape of padded sequences:\", padded_sequences.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data snippet: “Be yourself; everyone else is already taken.” “I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.” “Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.” “So many books, so little time.” “A room without books is like a body without a soul.” “Be who you are and say what you feel, because those who mind d...\n",
            "Number of sequences: 1\n",
            "Max sequence length: 80493\n",
            "Shape of padded sequences: (1, 80493)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming 'dataset' is the datasets object loaded from the JSONL file\n",
        "\n",
        "# Tokenize the quote column from the dataset\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(dataset['quote'])\n",
        "sequences = tokenizer.texts_to_sequences(dataset['quote'])\n",
        "\n",
        "# Find the maximum sequence length among all sequences\n",
        "max_sequence_len = max([len(x) for x in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Prepare input and target data from the padded sequences\n",
        "# For language modeling, we usually use the first tokens as input\n",
        "# and the last token as the target for each sequence.\n",
        "X, y = padded_sequences[:, :-1], padded_sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index)+1)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_sequence_len-1),\n",
        "    LSTM(150),\n",
        "    Dense(len(tokenizer.word_index)+1, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5MGgM6sXGLv",
        "outputId": "3873274b-f356-4577-f594-ab513c313ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.9053 - loss: 5.3040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ed9fdf00250>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        # Ensure the token list doesn't exceed the model's input length\n",
        "        # The model was trained with input_length=max_sequence_len-1\n",
        "        token_list = token_list[-(max_sequence_len - 1):]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0).argmax(axis=-1)\n",
        "        output_word = \"\"\n",
        "        # Find the word corresponding to the predicted index\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        # If the predicted index is 0 (padding index, which shouldn't happen with argmax on a trained model but good practice),\n",
        "        # or if no word is found (shouldn't happen if the model output is within tokenizer's vocabulary),\n",
        "        # we might want to handle this case. For now, we just append the output_word.\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Generate text using a different seed\n",
        "print(generate_text(\"The quick brown fox\", 20, max_sequence_len))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjiUNhv5X7Rk",
        "outputId": "94278622-24af-4712-9f12-abcce18b0271"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox ” ” ” ” ” ” ” ” ” ” ” ” ” ” ” ” ” ” ” ”\n"
          ]
        }
      ]
    }
  ]
}